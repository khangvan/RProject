---
title: "DataPipelineWorkflow"
author: "KhangVan"
date: "January 19, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## LOAD LIBRARY

```{r Load Library, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
library(tidyverse)
library(dplyr)
library(readxl) # for excel import
library(data.table) # for excel import
library(ggplot2)
install.packages("caret", dependencies = c("Depends", "Suggests"))
library(caret)
library(ellipse)
library(class)
```

```{r Excel Source, include=FALSE}
#load file excel
df <- read_excel("students.xls", col_names = TRUE)
#sumarise table
glimpse(df)
#load as datatablse
dft<-data.table(df)
#view as datatable
dft 

```

```{r Simple Linear Regression-Machine Learning w/R, include=FALSE}
#https://machinelearningmastery.com/machine-learning-in-r-step-by-step/

# attach the iris dataset to the environment
data(iris)
# rename the dataset
dataset <- iris
colnames(dataset) <- c("Sepal.Length","Sepal.Width","Petal.Length","Petal.Width","Species")
#view
dataset
summary(dataset)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Species, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]

dim(dataset)
dim(validation)
dim(validation_index)


# list types for each attribute
sapply(dataset, class)
head(dataset)

# list the levels for the class
levels(dataset$Species)

# summarize the class distribution
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq=table(dataset$Species), percentage=percentage)
summary(dataset)

# split input and output
x <- dataset[,1:4]
y <- dataset[,5]

# boxplot for each attribute on one image
par(mfrow=c(1,4))
  for(i in 1:4) {
  boxplot(x[,i], main=names(iris)[i])
  }

# barplot for class breakdown
plot(y)

# scatterplot matrix
featurePlot(x=x, y=y, plot="ellipse")


# density plots for each attribute by class value
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="density", scales=scales)


# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"


# a) linear algorithms
set.seed(7)
fit.lda <- train(Species ~., data=dataset, method="lda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- train(Species~., data=dataset, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- train(Species~., data=dataset, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- train(Species~., data=dataset, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(Species~., data=dataset, method="rf", metric=metric, trControl=control)


results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
```

https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/


## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
